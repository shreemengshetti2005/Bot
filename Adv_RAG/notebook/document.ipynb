{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df3518b",
   "metadata": {},
   "source": [
    "### Data Ingeshtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2dc3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25af996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/eg.txt'}, page_content='Hello, my name is Shree Mengshetti. I am applying for the AI Research Engineer Internship role. Please find my resume attached for your kind consideration.')]\n"
     ]
    }
   ],
   "source": [
    "# Single file load\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('../data/text_files/eg.txt', encoding='utf8')\n",
    "document= loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e2e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2084.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\eg.txt'}, page_content='Hello, my name is Shree Mengshetti. I am applying for the AI Research Engineer Internship role. Please find my resume attached for your kind consideration.'),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\ml.txt'}, page_content='Machine Learning (ML) is a branch of artificial intelligence that enables computers to learn from data and improve their performance on tasks without being explicitly programmed. Instead of following fixed instructions, ML algorithms identify patterns and make predictions or decisions based on input data.\\n\\nThere are three main types of ML:\\n\\nSupervised Learning – The model learns from labeled data (e.g., predicting house prices based on features like size and location).\\n\\nUnsupervised Learning – The model finds hidden patterns in unlabeled data (e.g., customer segmentation).\\n\\nReinforcement Learning – The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties (e.g., training robots or game AI).\\n\\nML is widely used in applications like recommendation systems, voice assistants, fraud detection, self-driving cars, and medical diagnosis.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple file load\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/text_files', \n",
    "    glob='**/*.txt',\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding': 'utf8'},\n",
    "    show_progress=True)\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ab804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\text_files\\\\eg.txt', 'file_path': '..\\\\data\\\\text_files\\\\eg.txt', 'total_pages': 1, 'format': 'Text', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Hello, my name is Shree Mengshetti. I am applying for\\nthe AI Research Engineer Internship role. Please find\\nmy resume attached for your kind consideration.'),\n",
       " Document(metadata={'producer': '', 'creator': '', 'creationdate': '', 'source': '..\\\\data\\\\text_files\\\\ml.txt', 'file_path': '..\\\\data\\\\text_files\\\\ml.txt', 'total_pages': 1, 'format': 'Text', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'encryption': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Machine Learning (ML) is a branch of artificial\\nintelligence that enables computers to learn from\\ndata and improve their performance on tasks without\\nbeing explicitly programmed. Instead of following\\nfixed instructions, ML algorithms identify patterns\\nand make predictions or decisions based on input\\ndata.\\nThere are three main types of ML:\\nSupervised Learning – The model learns from labeled\\ndata (e.g., predicting house prices based on features\\nlike size and location).\\nUnsupervised Learning – The model finds hidden\\npatterns in unlabeled data (e.g., customer\\nsegmentation).\\nReinforcement Learning – The model learns by\\ninteracting with an environment and receiving\\nfeedback in the form of rewards or penalties (e.g.,\\ntraining robots or game AI).\\nML is widely used in applications like recommendation\\nsystems, voice assistants, fraud detection, self-\\ndriving cars, and medical diagnosis.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/text_files', \n",
    "    glob='**/*.txt',\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    # loader_kwargs={'encoding': 'utf8'},\n",
    "    show_progress=True)\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152681bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 196\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# Example usage (uncomment to use):\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m# Simple usage (like Part 1)\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03mchunks = split_documents(all_pdf_documents)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m \u001b[33;03mnew_chunks = chunker.split_documents(other_documents)\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[43mchunks\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# Chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    Advanced document chunking using LangChain's RecursiveCharacterTextSplitter\n",
    "    with enhanced features and robust error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 1000,\n",
    "                 chunk_overlap: int = 200,\n",
    "                 separators: Optional[List[str]] = None,\n",
    "                 length_function: callable = len):\n",
    "        \"\"\"\n",
    "        Initialize the document chunker\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Maximum size of each text chunk\n",
    "            chunk_overlap: Number of characters to overlap between chunks\n",
    "            separators: List of separators for splitting. Defaults to comprehensive list\n",
    "            length_function: Function to measure text length\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    "        self.length_function = length_function\n",
    "        self.text_splitter = self._initialize_splitter()\n",
    "        \n",
    "    def _initialize_splitter(self) -> RecursiveCharacterTextSplitter:\n",
    "        \"\"\"Initialize the text splitter with configured parameters\"\"\"\n",
    "        try:\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "                separators=self.separators,\n",
    "                length_function=self.length_function\n",
    "            )\n",
    "            print(f\"✅ Text splitter initialized:\")\n",
    "            print(f\"   • Chunk size: {self.chunk_size}\")\n",
    "            print(f\"   • Overlap: {self.chunk_overlap}\")\n",
    "            print(f\"   • Separators: {self.separators[:3]}...\")\n",
    "            return splitter\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error initializing text splitter: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _ensure_document_objects(self, documents: Union[List[Document], List[str]]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Convert string inputs to Document objects if needed\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Document objects or raw strings\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            raise ValueError(\"Documents list cannot be empty\")\n",
    "            \n",
    "        # Check if first item is a string and convert all\n",
    "        if isinstance(documents[0], str):\n",
    "            print(f\"🔄 Converting {len(documents)} strings to Document objects...\")\n",
    "            return [Document(page_content=doc, metadata={}) for doc in documents]\n",
    "        \n",
    "        # Validate that all items are Document objects\n",
    "        for i, doc in enumerate(documents):\n",
    "            if not isinstance(doc, Document):\n",
    "                raise TypeError(f\"Item at index {i} is not a Document object or string\")\n",
    "                \n",
    "        return documents\n",
    "            \n",
    "    def split_documents(self, \n",
    "                       documents: Union[List[Document], List[str]], \n",
    "                       show_preview: bool = True,\n",
    "                       preview_length: int = 200) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split documents into smaller chunks\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Document objects or raw strings\n",
    "            show_preview: Whether to show example chunk preview\n",
    "            preview_length: Length of preview text to show\n",
    "            \n",
    "        Returns:\n",
    "            List of Document chunks\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure all inputs are Document objects\n",
    "            doc_objects = self._ensure_document_objects(documents)\n",
    "            \n",
    "            print(f\"📄 Processing {len(doc_objects)} documents for chunking...\")\n",
    "            \n",
    "            # Split documents into chunks\n",
    "            chunks = self.text_splitter.split_documents(doc_objects)\n",
    "            \n",
    "            print(f\"✅ Split {len(doc_objects)} documents into {len(chunks)} chunks\")\n",
    "            \n",
    "            # Show example chunk if requested and chunks exist\n",
    "            if show_preview and chunks:\n",
    "                self._show_chunk_preview(chunks[0], preview_length)\n",
    "            \n",
    "            # Show statistics\n",
    "            self._show_chunk_statistics(chunks)\n",
    "            \n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating chunks: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _show_chunk_preview(self, chunk: Document, preview_length: int = 200):\n",
    "        \"\"\"Show preview of first chunk\"\"\"\n",
    "        print(f\"\\n📋 Example chunk:\")\n",
    "        content_preview = chunk.page_content[:preview_length]\n",
    "        if len(chunk.page_content) > preview_length:\n",
    "            content_preview += \"...\"\n",
    "        print(f\"Content: {content_preview}\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "    \n",
    "    def _show_chunk_statistics(self, chunks: List[Document]):\n",
    "        \"\"\"Show chunk statistics\"\"\"\n",
    "        if not chunks:\n",
    "            return\n",
    "            \n",
    "        chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "        avg_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "        min_length = min(chunk_lengths)\n",
    "        max_length = max(chunk_lengths)\n",
    "        \n",
    "        print(f\"\\n📊 Chunk Statistics:\")\n",
    "        print(f\"   • Total chunks: {len(chunks)}\")\n",
    "        print(f\"   • Average length: {avg_length:.0f} characters\")\n",
    "        print(f\"   • Min length: {min_length} characters\")\n",
    "        print(f\"   • Max length: {max_length} characters\")\n",
    "    \n",
    "    def update_config(self, chunk_size: int = None, chunk_overlap: int = None):\n",
    "        \"\"\"Update chunker configuration and reinitialize\"\"\"\n",
    "        if chunk_size is not None:\n",
    "            self.chunk_size = chunk_size\n",
    "        if chunk_overlap is not None:\n",
    "            self.chunk_overlap = chunk_overlap\n",
    "            \n",
    "        self.text_splitter = self._initialize_splitter()\n",
    "        print(\"🔄 Chunker configuration updated\")\n",
    "\n",
    "\n",
    "# Initialize the chunker with default settings\n",
    "chunker = DocumentChunker(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Process the loaded documents\n",
    "chunks = chunker.split_documents(documents)  # Using the documents loaded earlier\n",
    "\n",
    "# You can also update the configuration if needed\n",
    "# chunker.update_config(chunk_size=800, chunk_overlap=100)\n",
    "# chunks = chunker.split_documents(documents)  # Re-process with new settings\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from the documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ccada",
   "metadata": {},
   "source": [
    "### Embedding and Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b96e099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Any, Dict, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72af6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1dabf32d550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        Args:\n",
    "        model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the Sentence Transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\") \n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        Args:\n",
    "        texts: List of strings to embed\n",
    "        Returns:\n",
    "        Numpy array of embeddings\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings    \n",
    "    \n",
    "## Initialize  the embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager() \n",
    "embedding_manager   \n",
    "    \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "505c886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection name: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1daff2f4ad0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Manages documents embedddings in a ChromaDB vector store\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/chroma_db\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "\n",
    "        Args:\n",
    "            collection_name (str, optional): Name of the document collection. Defaults to \"pdf_documents\".\n",
    "            persist_directory (str, optional): Directory to persist the vector store. Defaults to \"./chroma_db\".\n",
    "        \"\"\"\n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_vector_store()\n",
    "        \n",
    "        \n",
    "    def _initialize_vector_store(self):\n",
    "        \"\"\"Initialize the ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Create or get the collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF Document Embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection name: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise  \n",
    "        \n",
    "    def add_documents(self, documents: List[Any], embeddings :np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Document objects\n",
    "            embeddings: Corresponding numpy array of embeddings\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "        \n",
    "        # Prepare data for insertion in Chromadb\n",
    "        ids= []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc ,embedding) in enumerate(zip(documents, embeddings)):\n",
    "            \n",
    "            # Generate unique ID for each document\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata \n",
    "            metadata = dict(doc.metadata) \n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding \n",
    "            embeddings_list.append(embedding.tolist())\n",
    "            \n",
    "            # Add to Collection\n",
    "            try:\n",
    "                self.collection.add(\n",
    "                    ids=ids,\n",
    "                    embeddings=embeddings_list,\n",
    "                    metadatas=metadatas,\n",
    "                    documents=documents_text,\n",
    "                )\n",
    "                print(f\"Successfully added {len(documents)} documents to the vector store.\")\n",
    "                print(f\"Total documents in collection now: {self.collection.count()}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error adding documents to vector store: {e}\") \n",
    "                raise  \n",
    "        \n",
    "       \n",
    "        print(f\"Added {len(documents)} documents to the vector store. Total now: {self.collection.count()}\")\n",
    "        \n",
    "\n",
    "vectorStore=VectorStore()\n",
    "vectorStore        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fab455d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mchunks\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2d751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Adv_RAG (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
